# -*- coding: utf-8 -*-
"""Speaker Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Km82rIBideasb3C-f5lDbfi5EZqGztBS
"""

import os
import numpy as np
import scipy
from scipy.io import wavfile
import scipy.fftpack as fft
from scipy.signal import get_window
from scipy.signal import hamming
from __future__ import division 
import IPython.display as ipd
import matplotlib.pyplot as plt

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  ay=fn
  print(ay)

TRAIN_PATH = '/content/'
from zipfile import ZipFile
file_name = TRAIN_PATH +ay
with ZipFile(ay, 'r') as zip:
  zip.extractall()
  print('Done')

#TRAIN_PATH = '/content/cstr-vctk-filtered-mini/train/'

def read(num):
  sample_rate, audio = wavfile.read(TRAIN_PATH + "s" +str(num)+".wav")
  return [sample_rate, audio]





def mfcc(audio,sample_rate):
 def frame_audio(audio, FFT_size, hop_size,sample_rate):
    # hop_size in ms
    
    audio = np.pad(audio, int(FFT_size / 2), mode='constant')
    frame_len = np.round(sample_rate * hop_size / 1000).astype(int)
    frame_num = int((len(audio) - FFT_size) / frame_len) + 1
    frames = np.zeros((frame_num,FFT_size))
    
    for n in range(frame_num):
        frames[n] = audio[n*frame_len:n*frame_len+FFT_size]
    
    return frames
 hop_size = 15 #ms
 FFT_size = 2048 
 audio_framed = frame_audio(audio, FFT_size, hop_size,sample_rate)
 window = get_window("hann", FFT_size, fftbins=True) 
 audio_win = audio_framed * window
 audio_winT = np.transpose(audio_win)
 audio_fft = np.empty((int(1 + FFT_size // 2), audio_winT.shape[1]), dtype=np.complex64, order='F')
 
 for n in range(audio_fft.shape[1]):
    audio_fft[:, n] = fft.fft(audio_winT[:, n], axis=0)[:audio_fft.shape[0]]
 audio_fft = np.transpose(audio_fft)
 audio_power = np.square(np.abs(audio_fft))
 freq_min = 0
 freq_high = sample_rate / 2
 mel_filter_num = 10
 def freq_to_mel(freq):
    return 2595.0 * np.log10(1.0 + freq / 700.0)
 def met_to_freq(mels):
    return 700.0 * (10.0**(mels / 2595.0) - 1.0)
 def get_filter_points(fmin, fmax, mel_filter_num, FFT_size, sample_rate):
    fmin_mel = freq_to_mel(fmin)
    fmax_mel = freq_to_mel(fmax)
    
    #print("MEL min: {0}".format(fmin_mel))
    #print("MEL max: {0}".format(fmax_mel))
    
    mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num+2)
    freqs = met_to_freq(mels)
    return np.floor((FFT_size + 1) / sample_rate * freqs).astype(int), freqs

 filter_points, mel_freqs = get_filter_points(freq_min, freq_high, mel_filter_num, FFT_size, sample_rate)
 def get_filters(filter_points, FFT_size):
    filters = np.zeros((len(filter_points)-2,int(FFT_size/2+1)))
    
    for n in range(len(filter_points)-2):
        filters[n, filter_points[n] : filter_points[n + 1]] = np.linspace(0, 1, filter_points[n + 1] - filter_points[n])
        filters[n, filter_points[n + 1] : filter_points[n + 2]] = np.linspace(1, 0, filter_points[n + 2] - filter_points[n + 1])
    
    return filters
 filters = get_filters(filter_points, FFT_size)
 enorm = 2.0 / (mel_freqs[2:mel_filter_num+2] - mel_freqs[:mel_filter_num])
 filters *= enorm[:, np.newaxis]
 audio_filtered = np.dot(filters, np.transpose(audio_power))
 audio_log = 10.0 * np.log10(audio_filtered)
 def dct(dct_filter_num, filter_len):
    basis = np.empty((dct_filter_num,filter_len))
    basis[0, :] = 1.0 / np.sqrt(filter_len)
    
    samples = np.arange(1, 2 * filter_len, 2) * np.pi / (2.0 * filter_len)

    for i in range(1, dct_filter_num):
        basis[i, :] = np.cos(i * samples) * np.sqrt(2.0 / filter_len)
        
    return basis
 dct_filter_num = 40
 dct_filters = dct(dct_filter_num, mel_filter_num)
 cepstral_coefficents = np.dot(dct_filters, audio_log)
 #plt.figure(figsize=(15,5))
 #plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)
 #plt.imshow(cepstral_coefficents, aspect='auto', origin='lower')
 return cepstral_coefficents

#a=mfcc(audio,sample_rate)

def EUDistance(d,c):
  n = np.shape(d)[1]
  p = np.shape(c)[1]
  distance = np.empty((n,p))

  if n<p:
   for i in range(n):
    copies = np.transpose(np.tile(d[:,i], (p,1)))
    distance[i,:] = np.sum((copies - c)**2,0)
  else:
   for i in range(p):
    copies = np.transpose(np.tile(c[:,i],(n,1)))
    distance[:,i] = np.transpose(np.sum((d - copies)**2,0))
 
  distance = np.sqrt(distance)
  return distance
 
def lbg(features, M):
   eps = 0.01 
   codebook = np.mean(features, 1)
   distortion = 1
   nCentroid = 1
   while nCentroid < M:
     new_codebook = np.empty((len(codebook), nCentroid*2))
     if nCentroid == 1: 
      new_codebook[:,0] = codebook*(1+eps)
      new_codebook[:,1] = codebook*(1-eps)
     else:
      for i in range(nCentroid):
       new_codebook[:,2*i] = codebook[:,i] * (1+eps)
       new_codebook[:,2*i+1] = codebook[:,i] * (1-eps)

     codebook = new_codebook
     nCentroid = np.shape(codebook)[1]
     D = EUDistance(features, codebook)
   while np.abs(distortion) > eps:
      prev_distance = np.mean(D)
      nearest_codebook = np.argmin(D,axis = 1)
      for i in range(nCentroid):
       codebook[:,i] = np.mean(features[:,np.where(nearest_codebook == i)], 2).T

      codebook = np.nan_to_num(codebook)
      D = EUDistance(features, codebook)
      distortion = (prev_distance - np.mean(D))/prev_distance
   #print 'distortion' , distortion

   #print 'final codebook', codebook, np.shape(codebook)
   return codebook

TRAIN_PATH = '/content/cstr-vctk-filtered-mini/train/'
def train(num):
  temp=num+1
  final_codebook= np.empty((num+1,40, 8))
  for i in range(1,num+1):
    a=read(i)
    feat=mfcc(a[1],a[0])
    temp1=lbg(feat,5)
    final_codebook[i,:,:]=lbg(feat,5)
    #final_codebook.append(lbg(feat,5))
    #final_codebook.append(lbg(mfcc(())
  return final_codebook
num=8
Train_codebook=train(num)

def minDistance(features, codebooks):
 speaker = 1
 distmin = np.inf
 for k in range(num):
      D = EUDistance(features, codebooks[k])
      dist = np.sum(np.min(D, axis = 1))/(np.shape(D)[0])
      if dist < distmin:
       distmin = dist
       speaker = k
 return speaker



dict=[]
TRAIN_PATH = '/content/cstr-vctk-filtered-mini/test/'
dist_b=np.empty(num+1)
for i in range(1,num+1):
   a=read(i)
   feat=mfcc(a[1],a[0])
   
   dist_b[i-1]=np.linalg.norm((lbg(feat,5))-Train_codebook)
   dx=minDistance(feat,Train_codebook)
   k=1
   min=dist_b[1] 
   for p in range(num):
    if dist_b[p]<min:
      min=dist_b[p]  
      k=p
   print("speaker"+str(i)+"matches with"+str(dx))
   #print(k)
   #print(dist_b)



